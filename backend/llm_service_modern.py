"""
–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≤—Å–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ LLM API —Å retry –ª–æ–≥–∏–∫–æ–π, –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
"""
import aiohttp
import asyncio
import json
import time
from typing import List, Dict, Any, Optional, Union
from dataclasses import dataclass, asdict
from enum import Enum
from abc import ABC, abstractmethod
import hashlib
from loguru import logger
from config_simple import settings
from logging_config import log_execution

class LLMProvider(Enum):
    """–ü–µ—Ä–µ—á–∏—Å–ª–µ–Ω–∏–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    OPENAI = "openai"
    GEMINI = "gemini"
    ANTHROPIC = "anthropic"
    GROK = "grok"
    MISTRAL = "mistral"
    PERPLEXITY = "perplexity"

@dataclass
class LLMResponse:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM"""
    content: str
    provider: str
    model: str
    tokens_used: Optional[int] = None
    cost: Optional[float] = None
    response_time: Optional[float] = None
    error: Optional[str] = None
    cached: bool = False

@dataclass
class LLMRequest:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM"""
    prompt: str
    provider: LLMProvider
    model: Optional[str] = None
    max_tokens: Optional[int] = None
    temperature: float = 0.7
    system_prompt: Optional[str] = None

class BaseLLMProvider(ABC):
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.api_key = config.get('api_key')
        self.api_url = config.get('api_url')
        self.model = config.get('model')
        
        if not self.api_key:
            raise ValueError(f"API key is required for {self.__class__.__name__}")
    
    @abstractmethod
    async def make_request(self, request: LLMRequest) -> LLMResponse:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∫ LLM API"""
        pass

class OpenAIProvider(BaseLLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è OpenAI GPT"""
    
    async def make_request(self, request: LLMRequest) -> LLMResponse:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})
        
        data = {
            "model": request.model or self.model or "gpt-3.5-turbo",
            "messages": messages,
            "max_tokens": request.max_tokens or 1000,
            "temperature": request.temperature
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.api_url, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    return LLMResponse(
                        content=result["choices"][0]["message"]["content"],
                        provider="openai",
                        model=data["model"],
                        tokens_used=result.get("usage", {}).get("total_tokens")
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"OpenAI API error: {response.status} - {error_text}")

class GeminiProvider(BaseLLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Google Gemini"""
    
    async def make_request(self, request: LLMRequest) -> LLMResponse:
        url = f"{self.api_url}?key={self.api_key}"
        
        headers = {
            "Content-Type": "application/json"
        }
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è Gemini
        full_prompt = request.prompt
        if request.system_prompt:
            full_prompt = f"{request.system_prompt}\n\n{request.prompt}"
        
        data = {
            "contents": [{
                "parts": [{"text": full_prompt}]
            }],
            "generationConfig": {
                "temperature": request.temperature,
                "maxOutputTokens": request.max_tokens or 1000
            }
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    content = result["candidates"][0]["content"]["parts"][0]["text"]
                    return LLMResponse(
                        content=content,
                        provider="gemini",
                        model="gemini-pro"
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"Gemini API error: {response.status} - {error_text}")

class AnthropicProvider(BaseLLMProvider):
    """–ü—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Anthropic Claude"""
    
    async def make_request(self, request: LLMRequest) -> LLMResponse:
        headers = {
            "x-api-key": self.api_key,
            "Content-Type": "application/json",
            "anthropic-version": "2023-06-01"
        }
        
        data = {
            "model": request.model or self.config.get('model', 'claude-3-sonnet-20240229'),
            "max_tokens": request.max_tokens or 1000,
            "temperature": request.temperature,
            "messages": [
                {"role": "user", "content": request.prompt}
            ]
        }
        
        if request.system_prompt:
            data["system"] = request.system_prompt
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.api_url, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    return LLMResponse(
                        content=result["content"][0]["text"],
                        provider="anthropic",
                        model=data["model"],
                        tokens_used=result.get("usage", {}).get("input_tokens", 0) + 
                                   result.get("usage", {}).get("output_tokens", 0)
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"Anthropic API error: {response.status} - {error_text}")

class GenericProvider(BaseLLMProvider):
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è Grok, Mistral, Perplexity –∏ –¥—Ä—É–≥–∏—Ö OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã—Ö API"""
    
    async def make_request(self, request: LLMRequest) -> LLMResponse:
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        messages = []
        if request.system_prompt:
            messages.append({"role": "system", "content": request.system_prompt})
        messages.append({"role": "user", "content": request.prompt})
        
        data = {
            "model": request.model or self.model,
            "messages": messages,
            "max_tokens": request.max_tokens or 1000,
            "temperature": request.temperature
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(self.api_url, headers=headers, json=data) as response:
                if response.status == 200:
                    result = await response.json()
                    return LLMResponse(
                        content=result["choices"][0]["message"]["content"],
                        provider=request.provider.value,
                        model=data["model"],
                        tokens_used=result.get("usage", {}).get("total_tokens")
                    )
                else:
                    error_text = await response.text()
                    raise Exception(f"{request.provider.value} API error: {response.status} - {error_text}")

class LLMService:
    """–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞–º–∏"""
    
    def __init__(self):
        self.providers = {}
        self.cache = {}  # –ü—Ä–æ—Å—Ç–æ–π in-memory –∫–µ—à
        self.cache_ttl = 3600  # 1 —á–∞—Å
        self.session_timeout = aiohttp.ClientTimeout(total=60)
        self.max_retries = 3
        self.retry_delay = 1.0
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤
        self._init_providers()
    
    def _init_providers(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        configs = {
            LLMProvider.OPENAI: settings.get_llm_config('openai'),
            LLMProvider.GEMINI: settings.get_llm_config('gemini'),
            LLMProvider.ANTHROPIC: settings.get_llm_config('anthropic'),
            LLMProvider.GROK: settings.get_llm_config('grok'),
            LLMProvider.MISTRAL: settings.get_llm_config('mistral'),
            LLMProvider.PERPLEXITY: settings.get_llm_config('perplexity'),
        }
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Ö, —É –∫–æ–≥–æ –µ—Å—Ç—å API –∫–ª—é—á–∏
        for provider, config in configs.items():
            if config.get('api_key'):
                try:
                    if provider == LLMProvider.OPENAI:
                        self.providers[provider] = OpenAIProvider(config)
                    elif provider == LLMProvider.GEMINI:
                        self.providers[provider] = GeminiProvider(config)
                    elif provider == LLMProvider.ANTHROPIC:
                        self.providers[provider] = AnthropicProvider(config)
                    else:
                        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä –¥–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
                        self.providers[provider] = GenericProvider(config)
                    
                    logger.info(f"‚úÖ Initialized {provider.value} provider")
                except Exception as e:
                    logger.error(f"‚ùå Failed to initialize {provider.value} provider: {e}")
            else:
                logger.warning(f"‚ö†Ô∏è {provider.value} provider not configured (missing API key)")
    
    def _get_cache_key(self, request: LLMRequest) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–ª—é—á–∞ –∫–µ—à–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        request_str = f"{request.provider.value}:{request.prompt}:{request.model}:{request.temperature}"
        return hashlib.md5(request_str.encode()).hexdigest()
    
    def _get_from_cache(self, cache_key: str) -> Optional[LLMResponse]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –∏–∑ –∫–µ—à–∞"""
        if cache_key in self.cache:
            cached_data, timestamp = self.cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                cached_response = LLMResponse(**cached_data)
                cached_response.cached = True
                return cached_response
            else:
                # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫–µ—à
                del self.cache[cache_key]
        return None
    
    def _save_to_cache(self, cache_key: str, response: LLMResponse):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∫–µ—à"""
        # –ù–µ –∫–µ—à–∏—Ä—É–µ–º –æ—à–∏–±–∫–∏
        if not response.error:
            self.cache[cache_key] = (asdict(response), time.time())
    
    @log_execution("LLM Request")
    async def make_request_with_retry(self, request: LLMRequest) -> LLMResponse:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å retry –ª–æ–≥–∏–∫–æ–π"""
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
        cache_key = self._get_cache_key(request)
        cached_response = self._get_from_cache(cache_key)
        if cached_response:
            logger.debug(f"üì¶ Cache hit for {request.provider.value}")
            return cached_response
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞
        if request.provider not in self.providers:
            return LLMResponse(
                content="",
                provider=request.provider.value,
                model="unavailable",
                error=f"Provider {request.provider.value} not configured"
            )
        
        provider = self.providers[request.provider]
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å —Å retry
        last_error = None
        for attempt in range(self.max_retries):
            try:
                start_time = time.time()
                response = await provider.make_request(request)
                response.response_time = time.time() - start_time
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                self._save_to_cache(cache_key, response)
                
                logger.debug(f"‚úÖ {request.provider.value} request successful in {response.response_time:.2f}s")
                return response
                
            except Exception as e:
                last_error = str(e)
                logger.warning(f"‚ö†Ô∏è {request.provider.value} attempt {attempt + 1} failed: {e}")
                
                if attempt < self.max_retries - 1:
                    await asyncio.sleep(self.retry_delay * (2 ** attempt))  # Exponential backoff
        
        # –í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –Ω–µ—É–¥–∞—á–Ω—ã
        logger.error(f"‚ùå {request.provider.value} failed after {self.max_retries} attempts")
        return LLMResponse(
            content="",
            provider=request.provider.value,
            model="error",
            error=last_error
        )
    
    async def get_serp_data(self, word: str, provider: LLMProvider) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ SERP –¥–∞–Ω–Ω—ã—Ö –æ—Ç —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞"""
        system_prompt = """
–¢—ã –ø–æ–∏—Å–∫–æ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞. –î–ª—è –¥–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –≤—ã–¥–∞–π —Ç–æ–ø-10 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
1. –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
2. –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
...

–í–∫–ª—é—á–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏, –±—Ä–µ–Ω–¥—ã –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å —ç—Ç–∏–º –∑–∞–ø—Ä–æ—Å–æ–º.
–û—Ç–≤–µ—á–∞–π —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–∫–æ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤.
"""
        
        request = LLMRequest(
            prompt=f"–ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å: {word}",
            provider=provider,
            system_prompt=system_prompt,
            temperature=0.7,
            max_tokens=1000
        )
        
        response = await self.make_request_with_retry(request)
        return response.content if not response.error else f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –æ—Ç {provider.value}: {response.error}"
    
    async def extract_companies_from_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –Ω–∞–∏–±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ LLM"""
        
        # –í—ã–±–∏—Ä–∞–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        available_providers = [p for p in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC, LLMProvider.GEMINI] 
                             if p in self.providers]
        
        if not available_providers:
            logger.warning("‚ö†Ô∏è No LLM providers available for company extraction")
            return self._extract_companies_fallback(text)
        
        provider = available_providers[0]
        
        system_prompt = """
–ò–∑ –¥–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑–≤–ª–µ–∫–∏ –≤—Å–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π, —Ç–æ–≤–∞—Ä–æ–≤, –±—Ä–µ–Ω–¥–æ–≤ –∏ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –Ω–∞–∑–≤–∞–Ω–∏–π.
–í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é, –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π.
–ï—Å–ª–∏ –∫–æ–º–ø–∞–Ω–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–µ—Ä–Ω–∏ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.
"""
        
        request = LLMRequest(
            prompt=f"–¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:\n{text}",
            provider=provider,
            system_prompt=system_prompt,
            temperature=0.3,
            max_tokens=200
        )
        
        response = await self.make_request_with_retry(request)
        
        if response.error:
            logger.warning(f"‚ö†Ô∏è Company extraction failed, using fallback method")
            return self._extract_companies_fallback(text)
        
        # –ü–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        companies_text = response.content.strip()
        if not companies_text:
            return []
        
        companies = [company.strip() for company in companies_text.split(',')]
        return [c for c in companies if c and len(c) > 2][:10]  # –§–∏–ª—å—Ç—Ä—É–µ–º –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
    
    def _extract_companies_fallback(self, text: str) -> List[str]:
        """Fallback –º–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        companies = []
        keywords = ["–∫–æ–º–ø–∞–Ω–∏—è", "–±—Ä–µ–Ω–¥", "—Å–µ—Ä–≤–∏—Å", "–º–∞–≥–∞–∑–∏–Ω", "–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞", "–∫–æ—Ä–ø–æ—Ä–∞—Ü–∏—è"]
        lines = text.split('\n')
        
        for line in lines:
            for keyword in keywords:
                if keyword.lower() in line.lower():
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ –∫–ª—é—á–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                    parts = line.split(' - ')
                    if len(parts) > 0:
                        company_name = parts[0].strip().split('. ')[-1]
                        if company_name and len(company_name) > 2:
                            companies.append(company_name)
        
        return list(set(companies))[:10]  # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º
    
    def get_available_providers(self) -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        return [provider.value for provider in self.providers.keys()]
    
    def get_provider_status(self) -> Dict[str, bool]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        status = {}
        for provider in LLMProvider:
            status[provider.value] = provider in self.providers
        return status

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
llm_service = LLMService()
